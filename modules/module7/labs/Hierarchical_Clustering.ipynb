{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Here, we do not know in advance how many clusters we want. Once the algorithm is run, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n.\n",
    "\n",
    "K-means clustering requires us to pre-specify the number of clusters K which is not a requirement for Hierarchical clustering. Also, it has an added advantage that it results in an attractive tree-based representation of the observations at different scales of clustering, called a dendrogram. \n",
    "\n",
    "Bottom-up or agglomerative clustering is the most common type of hierarchical clustering, and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk. \n",
    "\n",
    "\n",
    "In the image shown below, each leaf of the dendrogram represents an observation. As we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. For any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different. \n",
    "\n",
    "<img src=\"../images/dendogram.JPG\">\n",
    "\n",
    "The dashed line represents the cut. The left image is not cut which gives us one cluster for all observations. The middle image is cut in such a way that two clusters are generated. The cut on right image generates 3 clusters. \n",
    "\n",
    "## Hierarchical Clustering Algorithm\n",
    "\n",
    "A dissimilarity measure, most often Euclidean distance, is used between each pair of observations. Starting out at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there are now n−1 clusters. Next, the two clusters that are most similar to each other are fused again, so that there are now n−2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.\n",
    "\n",
    "The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations. The four most common types of linkage are complete, average, single, and centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Generate the data to perform hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the seed to reproduce the results\n",
    "set.seed(4)\n",
    "\n",
    "# Generate a random normal distribution of 100 values. Generate a matrix out of this normal distribution.\n",
    "x=matrix(rnorm(50*2), ncol=2)\n",
    "\n",
    "# Add 3 to rows 1 to 25 in first column of the matrix\n",
    "x[1:25, 1]=x[1:25, 1] + 3\n",
    "\n",
    "# Subtract 4 from rows 1 to 25 in second column of the matrix\n",
    "x[1:25, 2]=x[1:25, 2] - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "The hclust() function is used to implement hierarchical clustering in R. Create matrix x similar to K-Means lab. Use that data to plot the hierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. \n",
    "\n",
    "|        |Linkage Description|\n",
    "|--------|---------------------------------------------------------------------------------------------------------------------|\n",
    "|**Complete**|Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.                                                    |\n",
    "|**Single**  |Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.                                                                 |\n",
    "|**Average** |Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.                                                    |\n",
    "|**Centroid**|Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.                                                                                  |\n",
    "\n",
    "\n",
    "Begin by clustering observations using complete linkage. Use dist() function to compute the 50 × 50 inter-observation Euclidean distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "hc.complete = hclust(dist(x), method=\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform hierarchical clustering with average and single linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hc.average = hclust(dist(x), method =\"average\")\n",
    "hc.single = hclust(dist(x), method =\"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the dendrograms obtained using the plot() function. The numbers at the bottom of the plot identify each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Arrange 3 plots in a row\n",
    "par(mfrow =c(1,3))\n",
    "\n",
    "# Plot all the 3 dendograms generated using different linkage schemes.\n",
    "plot(hc.complete ,main =\" Complete Linkage \", xlab=\"\", sub =\"\", cex =.9)\n",
    "plot(hc.average , main =\" Average Linkage \", xlab=\"\", sub =\"\", cex =.9)\n",
    "plot(hc.single , main=\" Single Linkage \", xlab=\"\", sub =\"\", cex =.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the cutree() function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutree(hc.complete, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutree(hc.average, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutree(hc.single, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the variables before performing hierarchical clustering of the observations, we use the scale() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xsc=scale(x)\n",
    "plot(hclust(dist(xsc), method=\"complete\"), main =\" Hierarchical Clustering with Scaled Features \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation-based distance can be computed using the as.dist() function, which converts an arbitrary square symmetric matrix into a form that the hclust() function recognizes as a distance matrix. However, this only makes sense for data with at least three features since the absolute correlation between any two observations with measurements on two features is always 1. Hence, we will cluster a three-dimensional data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=matrix(rnorm(30*3), ncol=3)\n",
    "dd=as.dist(1-cor(t(x)))\n",
    "plot(hclust(dd, method=\"complete\"), main=\"Complete Linkage with Correlation -Based Distance \", xlab=\"\", sub =\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisive Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#install.packages(\"cluster\",repo=\"https://cran.mtu.edu/\")\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute diana()\n",
    "# x : could be matrix or data frame, or dissimilarity matrix or object.\n",
    "# metric: character string specifying the metric to be used for calculating dissimilarities between observations.\n",
    "# The currently available options are \"euclidean\" and \"manhattan\"\n",
    "\n",
    "res_diana <- diana(x, metric = \"euclidean\", stand = FALSE)\n",
    "print(res_diana)\n",
    "# Plot the tree\n",
    "pltree(res_diana, cex = 0.6, hang = -1, main = \"Dendrogram of diana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Cut into 2 groups:\n",
    "diana2 <- cutree(as.hclust(res_diana), k = 2)\n",
    "\n",
    "table(diana2) # 8 and 42 group members\n",
    "\n",
    "rownames(x)[diana2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divise coefficient; amount of clustering structure found\n",
    "res_diana$dc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
