{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "In this practice, we will apply the k-means algorithm to the same toy data sets we have created for the previous module. We will see how k-means algorithm behaves for different densities of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from the file\n",
    "data1 <- read.csv(\"../../../datasets/toydata/data1.csv\",header=TRUE)\n",
    "\n",
    "str(data1)\n",
    "# Visualize the data\n",
    "library(ggplot2)\n",
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plotted the data with the given classes as before. But we will assume we don't know the classes, because **clustering** is one of the *unsupervised* learning approaches; we look at the data without the class information now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(colour=\"black\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's apply K-Means to this data set; when we plot the data we see clearly there are two clusters; so let's start with two. \n",
    "set.seed(42)\n",
    "kmeans_clust11 <- kmeans(data1[, 1:2], 2, nstart=20)\n",
    "kmeans_clust11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's compare the cluster assignments to the actual class labels, and plot the data with the k-means cluster labels\n",
    "table(kmeans_clust11$cluster, data1$class)\n",
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust11$cluster))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, k-means does a good job in this simple data set. Again, remember that we tell the algorithm how many clusters there are. And we know that because we can visualize the data and have an idea about the clusters. If we couldn't do that, we would have to make up a number and try different numbers of clusters until we feel we have the right number. Let's try the k-means with a different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try 3 clusters\n",
    "kmeans_clust12 <- kmeans(data1[, 1:2], 3, nstart=20)\n",
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust12$cluster))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means created this somewhat of an unnatural clustering because we asked to have three clusters. The main drawback of the k-means algorithm is that we need to have a good idea of number of clusters to begin with. \n",
    "\n",
    "Let's apply it to the second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from the file \n",
    "data2 <- read.csv(\"../../../datasets/toydata/data2.csv\",header=TRUE)\n",
    "\n",
    "# Visualize the data\n",
    "pl2 <- ggplot(data2, aes(X, Y)) + geom_point(colour=\"black\")\n",
    "pl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try 2 clusters\n",
    "kmeans_clust21 <- kmeans(data2[, 1:2], 2, nstart=20)\n",
    "pl1 <- ggplot(data2, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust21$cluster))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  see how well it does; compute the confusion given the actual labels. **Now it's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table(<what goes in here>, <what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means does a pretty good job given that the clusters in this data set are overlapped. Let's try with different numbers. **Now it's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try 3 clusters\n",
    "kmeans_clust22 <- kmeans(<what goes in here>)\n",
    "pl1 <- ggplot(data2, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust22$cluster))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try 4 clusters\n",
    "kmeans_clust23 <- kmeans(<what goes in here>)\n",
    "pl1 <- ggplot(data2, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust23$cluster))) + theme(legend.position=\"top\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar visualization by using the *fviz_cluster* function from \n",
    "**factoextra** library that has useful functions to deal with clustering related programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(factoextra)\n",
    "# visualize clusters and fit an ellipse \n",
    "fviz_cluster(kmeans_clust23, data = data2[, 1:2], geom = \"point\", stand = FALSE, frame.type = \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is really no right or wrong answer here; some of these clusters look *OK*, they do not look unnatural. So is there a way of finding an **optimal** number of clusters? \n",
    "\n",
    "One method is to run the clustering algorithm for different values of k such as by varying k from 1 to 10 clusters, and for \n",
    "each k, calculate the total within-cluster sum of square (wss) and plot the curve of wss according to the number of clusters k.\n",
    "The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute and plot wss for k = 1 to k = 10\n",
    "k.max <- 10 # Maximal number of clusters\n",
    "# grab the data \n",
    "data <- data2[, 1:2]\n",
    "# apply different values of k to kmeans function and collect tot.withinss for each k. \n",
    "# tot.withinss : Total within-cluster sum of squares\n",
    "wss <- sapply(1:k.max,function(k){kmeans(data, k, nstart=20 )$tot.withinss})\n",
    "# plot k versus tot.withinss\n",
    "plot(1:k.max, wss, type=\"b\", pch = 19, frame = FALSE, xlab=\"Number of clusters K\", ylab=\"tot.withinss\")\n",
    "\n",
    "# visually, 2 seems to be a good number for clusters.\n",
    "abline(v=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one of the possible ways we can evaluate the best number of clusters. We can compute a number of measures by using the **NbClust** library, and pick the number of clusters that is suggested by the majority of all different measures it computes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(NbClust)\n",
    "nb <- NbClust(data, distance=\"euclidean\", min.nc=2, max.nc=10, method=\"complete\", index=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also tells us the best number of clusters to be two. Let's look at the different methods and their suggested number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb$Best.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it seems most of them suggest cluster number to be two. We can also visualize the nbclust result using the factoextra library's function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fviz_nbclust(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same approach for the third data set. From previous module, we know that this data set was produced by creating six normally distributed clusters. Let's see if we can find the best number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from file\n",
    "data3 <- read.csv(\"../../../datasets/toydata/data3.csv\",header=TRUE)\n",
    "pl3 <- ggplot(data3, aes(X, Y)) + geom_point(colour=\"black\")\n",
    "pl3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try a few random numbers. Start with 4. **It's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try 4 clusters\n",
    "kmeans_clust31 <- kmeans(<what goes in here>)\n",
    "pl3 <- ggplot(data3, aes(X, Y)) + geom_point(aes(colour=factor(kmeans_clust31$cluster))) + theme(legend.position=\"top\")\n",
    "pl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's plot with the fviz_cluster function\n",
    "fviz_cluster(kmeans_clust31, <what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the NbClust to find out the best number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb3 <- NbClust(<what goes in here>, distance=\"euclidean\", min.nc=2, max.nc=10, method=\"complete\", index=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's visualize it with fviz_nbclust\n",
    "fviz_nbclust(nb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. Most of the methods suggest 3 as the number of clusters, but 6 is a very close second, and we know 6 should be the \"right\" answer. By looking at this plot and the actual clusters, we can decide whether this data set should be analyzed by separating it to 3 clusters or 6 clusters. Now, let's find the clusters and plot them. **Now it's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply k-means to data3 for three clusters \n",
    "kmeans_clust32 <- <what goes in here>\n",
    "# plot the clusters with fviz_cluster\n",
    "fviz_cluster(<what goes in here>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply k-means to data3 for six clusters \n",
    "kmeans_clust33 <- <what goes in here>\n",
    "# plot the clusters with fviz_cluster\n",
    "fviz_cluster(<what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Both results seem to be fine. If we want to analyze the data in a coarser scale, we can pick 3 as the number of clusters, otherwise we can pick 6 for finer scale. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
