{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "\n",
    "Random Forest is the choice of algorithm when one can’t think of any algorithm irrespective of situation, to apply on a dataset or if one wants to learn about the data before applying any more apt complex algorithms. It is considered to be a solution of all data science problems. \n",
    "\n",
    "Random Forests are capable of performing both regression and classification tasks. It helps in dimensional reduction, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. Before trying to get into the details of random forest understanbd how decision trees work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees** are a type of supervised learning algorithm mostly used in classification problems. It works for both categorical and continuous input and output variables. The main idea behind algorithm is to split the population or sample into two or more sub-populations based on a most significant differentiator in input variables.\n",
    "\n",
    "<img src=\"../images/decision_tree.png\">\n",
    "\n",
    "\n",
    "image source: [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/)\n",
    "\n",
    "**Example:** Consider a problem of predicting whether a customer will pay his loan debt amount either yes or no. The income of the customer is the deciding variable in this case. But the company doesn't income details of all customers. Based on the insight that incomes drives this decision a decision tree can be built to predict customer's income based on occupation, education level and sex and various other variables. Here continuous variable is being predicted. \n",
    "\n",
    "Decision tree builds a single tree whether if its classification or regression using CART model() but Random forest algorithm builds multiple trees. A random forest to classify an object based on attributes, each tree that is built gives a classification and votes for a class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.\n",
    "\n",
    "Illustrate this idea on iris dataset and compare the results. Load the default iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data=(data=iris)\n",
    "head(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visually inspect the data on a graph\n",
    "\n",
    "library(ggplot2)\n",
    "qplot(Petal.Length,Petal.Width,colour=Species,data=iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install below packages for building a CART model.\n",
    "library(rpart)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference: ** \n",
    "\n",
    "- [rpart](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)\n",
    "- [caret](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)\n",
    "- [Tree based models](http://www.statmethods.net/advstats/cart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the population in to training and testing sets. Compare the predictive power of decision tree and random forest on testing set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a vector called flag such that 70% of the data is put into training set and rest in to testing set. \n",
    "# flag will have row numbers corresponding to observations that will be put into training set and the rows remaining in iris_data\n",
    "# will be put into testing set.\n",
    "flag <- createDataPartition(y=iris_data$Species,p=0.7,list=FALSE)\n",
    "\n",
    "# training will have rows from iris_data for the row numbers present in flag vector.\n",
    "training <- iris_data[flag,]\n",
    "nrow(training)\n",
    "\n",
    "# testing will have rows from iris_data which are not present in flag vector.\n",
    "testing <- iris_data[-flag,]\n",
    "nrow(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 105 observations in training set and 45 in testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a CART model. \"caret\" and \"rpart\" packages will be used to build the model. To create a more graphically appealing graph in R, a package called “rattle” is used to make the decision tree. \"Rattle\" builds more fancy and clean trees which are easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"rpart.plot\",repo=\"http://cran.mtu.edu/\")\n",
    "\n",
    "fit <- train(Species~.,method=\"rpart\",data=training)\n",
    "\n",
    "# Code for generating decision tree plot\n",
    "# rpart_fit <- rpart(Species~.,method=\"class\",data=training) \n",
    "# library(rpart.plot)\n",
    "# rpart.plot(rpart_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now check the predictive power of the CART model that is just built. Check for the number of misclassifications in the tree as the decision criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.pred<-predict(fit,newdata=training)\n",
    "table(train.pred,training$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Misclassification rate = 4/105\n",
    "4/105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 4 misclassifications out of 105 observations. The misclassification rate signifies its predictive power. Once the model is built, it should be validated on a test set to see how well it performs on unknown data. This will help in determining the model is not over fitted. In case the model is over fitted, validation will show a sharp decline in the predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.pred<-predict(fit,newdata=testing)\n",
    "table(test.pred,testing$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Misclassification rate = 3/45\n",
    "3/45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The predictive power decreased in testing set as compared to training. This is generally true in most cases. The reason being, the model is trained on the training data set, and just overlaid on validation training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Run random forest algorithm on iris_data to compare the results with CART model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(randomForest)\n",
    "# library(randomForestSRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RandomForest_fit <- randomForest(Species~.,method=\"class\",data=training,importance=TRUE) \n",
    "\n",
    "\n",
    "plot(RandomForest_fit)\n",
    "legend(\"topright\", colnames(RandomForest_fit$err.rate),col=1:4,cex=0.8,fill=1:4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the amount of error with the variation in the number of trees constructed. Play with number of trees to generate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varImpPlot(RandomForest_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini importance: \n",
    "\n",
    "Every time a split of a node is made on variable m the gini impurity criterion for the two descendent nodes is less than the parent node. Adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.\n",
    "\n",
    "**Reference: **[Variable importance](https://en.wikipedia.org/wiki/Random_forest#Variable_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance(RandomForest_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"party\",repo=\"http://cran.mtu.edu/\")\n",
    "\n",
    "# library(party)\n",
    " \n",
    "# ct = ctree(Species~., data = training)\n",
    "# plot(ct, main=\"Tree\")\n",
    " \n",
    "# #Table of prediction errors\n",
    "# table(predict(ct), training$Species)\n",
    " \n",
    "# # Estimated class probabilities\n",
    "# train.pred = predict(ct, newdata=training, type=\"prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_fit <- train(Species~ .,method=\"rf\",data=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_RF_pred <- predict(RF_fit,training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table(train_RF_pred,training$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassification rate in training data is 0/105. Validate to make sure that the model is not over fitted on the training data by testing on tets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_RF_pred<-predict(RF_fit,newdata=testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table(test_RF_pred,testing$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 misclassified observations out of 45, which is similar to CART model prediction power. There is a significant drop in predictive power of the model when compared to training misclassification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Reduction using ANOVA, MANOVA and Random Forests.**\n",
    "\n",
    "Apply Random Forests and the techniques we have seen in other lab notebooks on bikeshare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bikeshare_data = read.csv(\"../../../datasets/bikeshare/hour.csv\")\n",
    "head(bikeshare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(bikeshare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bikeshare_data$hr = factor(bikeshare_data$hr)\n",
    "bikeshare_data$weekday = factor(bikeshare_data$weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- aov(mnth ~ hr, data=bikeshare_data)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value suggests there is no variation in the means of data by hour for all the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- aov(mnth ~ weekday, data=bikeshare_data)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- aov(temp ~ hr, data=bikeshare_data)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do pairwise comparison between group means for each hour\n",
    "pairwise.t.test(bikeshare_data$temp, bikeshare_data$hr,p.adjust=\"bonferroni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is not much variation in the temparature for some of the hours there is a lot of variation in the temparature of the day based on hour for some of the hours. Lets find the mean temparature of each hour using tapply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t(tapply(bikeshare_data$temp,bikeshare_data$hr,mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a MANOVA on variables temp,hum,windspeed,holiday and weathersit using hr and weekday variables\n",
    "summary(manova(cbind(temp,hum,windspeed,holiday,weathersit) ~ hr * weekday,\n",
    "               data = bikeshare_data), test = \"Hotelling-Lawley\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to manova, these variables temp,hum,windspeed,holiday and weathersit vary by hr and weekday. Lets analyze the same for rest of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names(bikeshare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(manova(cbind(season,mnth) ~ hr * weekday,\n",
    "               data = bikeshare_data), test = \"Hotelling-Lawley\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "month and season are not contributing anything as they have little variation in their data. \n",
    "\n",
    "### Measuring variable importance using Random Forests\n",
    "\n",
    "#### Gini importance\n",
    "\n",
    "The mean Gini gain that is produced by a feature over all trees. Consider `RF` is the Random Forest model fitted on the data. \n",
    "\n",
    "$RF <- randomForest(..., importance=TRUE)$\n",
    "\n",
    "There are 2 ways of checking the impoortance\n",
    "\n",
    "* RF$importance       **column**: MeanDecreaseGini\n",
    "\n",
    "* importance(RF, type=2)\n",
    "\n",
    "Note: For variables of different types: there will be a bias in favor of continuous variables and variables with many categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance\n",
    " \n",
    "The mean decrease in classification accuracy after permuting the feature over all trees \n",
    "\n",
    "$RF <- randomForest(..., importance=TRUE)$\n",
    "\n",
    "- RF$importance **column**: MeanDecreaseAccuracy\n",
    "- importance(RF, type=1)\n",
    "\n",
    "obj <- cforest(...)\n",
    "varimp(obj)\n",
    "\n",
    "Note: For variables of different types: unbiased only when subsampling is used as in cforest(..., controls = cforest unbiased())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a model across all the training data and plot the variable importance\n",
    "rf <- randomForest(bikeshare_data[,c(\"season\",\"holiday\",\"workingday\",\"weathersit\",\"temp\",\"atemp\",\"hum\",\"windspeed\",\"hr\")], \n",
    "                                  bikeshare_data$count, ntree=50, importance=TRUE)\n",
    "imp <- importance(rf, type=2)\n",
    "Imp_features <- data.frame(Feature=row.names(imp), Importance=imp[,1])\n",
    "\n",
    "p <- ggplot(Imp_features, aes(x=reorder(Feature, Importance), y=Importance)) +\n",
    "     geom_bar(stat=\"identity\", fill=\"blue\") +\n",
    "     coord_flip() + \n",
    "     theme_light(base_size=20) +\n",
    "     xlab(\"Importance\") +\n",
    "     ylab(\"\") + \n",
    "     ggtitle(\"Random Forest Feature Importance\\n\") +\n",
    "     theme(plot.title=element_text(size=18))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cbind(importance(rf, type=1),importance(rf, type=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varImpPlot(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the plots hr is the most important variable followed by holiday, atemp and so on for MeanDEcreaseAccuracy measure of importance. hum is the most important variable according to MeanDecreaseGini measure. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
