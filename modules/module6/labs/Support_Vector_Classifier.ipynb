{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "SVM in short is a generalization of a simple and intuitive classifier called the maximal margin classifier. maximal margin classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. An extension to maximal margin classifier called support vector classifier can be applied in a broader range of cases but cannot handle non-linear cases. SVM is an extension of the support vector classifier in order to accommodate non-linear class boundaries. Support vector machines are intended for the binary classification setting in which there are two classesbiut can be applied to the case of more than two classes.\n",
    "\n",
    "#### Hyperplane?\n",
    "\n",
    "In a p-dimensional space, a hyperplane is a subspace of dimension P − 1. In two dimensions, a hyperplane is a flat one-dimensional subspace i'e  a line. In three dimensions, hyperplane is a flat 2-d subspace a plane. In p > 3 dimensions, it can be hard to visualize a hyperplane. In two dimensions, a hyperplane is defined by the equation $$β_0 + β_1X_1 + β_2X_2 = 0$$\n",
    "\n",
    "Above equation defines the hyperplane. We say any point $X = (X_1,X_2)^T$ that holds above equation is a point on the hyperplane. Above equation is simply the equation of a line, since in two dimensions a hyperplane is a line. Above equation when extended to p-dimensions gives us \n",
    "\n",
    "$$β_0 + β_1X_1 + β_2X_2 + . . . + β_pX_p = 0$$\n",
    "\n",
    "any point $X = (X_1,X_2,....,β_pX_p)^T$ that holds above equation is a point on the hyperplane. If the point doesn't satisfy the equation and is greater than above equation $β_0 + β_1X_1 + β_2X_2 + . . . + β_pX_p > 0$ then X lies to one side of the hyperplane or the other side of plane if its less than the equation.\n",
    "\n",
    "<img src=\"../images/svm.JPG\">\n",
    "\n",
    "\n",
    "In the left image there are two classes of observations, shown in blue and in purple, each of which has measurements on two variables. Three separating hyperplanes, out of many possible, are shown in black. \n",
    "\n",
    "In the right image a separating hyperplane is shown in black. The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane. The blue region is the set of points for which line equation is evaluated as > 0, and the purple region is the set of points for which line equation is evaluated as < 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../images/hyperplane.JPG\">\n",
    "\n",
    "### Maximum margin classifier\n",
    "\n",
    "The maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes. In above picture the three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines\n",
    "indicating the width of the margin. These three observations are known as support vectors, since they are vectors in p-dimensional space and they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. \n",
    "\n",
    "### Support Vector Classifier\n",
    "\n",
    "<img src=\"../images/classifier.JPG\">\n",
    "\n",
    "The left most figure above shows that the observations that belong to two classes are not necessarily separable by a hyperplane. A classifier based on a separating hyperplane which tries to perfectly classify all of the training observations leads to sensitivity to individual observations. \n",
    "\n",
    "As shown in the image towards right the addition of a single observation compared to middle image leads to a dramatic change in the maximal margin hyperplane. It has only a tiny margin. This is problematic because the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified. Also, the fact that the maximal margin hyperplane is extremely sensitive to a change in a single observation suggests that it may have overfit the training data.\n",
    "\n",
    "Thus it is worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations. The **support vector classifier**, sometimes called a soft margin classifier does exactly this. Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but\n",
    "also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum margin classifier and Support vector classifier can take care of linear decision boundaries. To handle non linear decision boundaries we need **support vector machines**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "<img src=\"../images/non_linear_boundary.JPG\">\n",
    "\n",
    "If you look at above picture the observations fall into two classes, with a non-linear boundary between them. Picture on the right shows a support vector classifier that seeks a linear boundary and consequently performs very poorly. \n",
    "\n",
    "When there is a nonlinear relationship between the predictors and the outcome we consider enlarging the feature space using functions of the predictors such as quadratic and cubic terms, in order to address this non-linearity. In the enlarged feature space, the decision boundary is in fact linear. But in the original feature space, the decision boundary is of the form q(x) = 0, where q is a quadratic polynomial, and its solutions are generally non-linear.\n",
    "\n",
    "\n",
    "When the support vector classifier is combined with a non-linear kernel, the resulting classifier is known as a support vector machine. A kernel is a function that quantifies the similarity of two observations. Using a kernal amounts to fitting a support vector classifier in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space. \n",
    "\n",
    "\n",
    "\n",
    "The image on the left below shows an example of an SVM with a polynomial kernel of degree 3 is applied to the non-linear data shown in above image. The fit is a substantial improvement over the linear support vector classifier and is a far more appropriate decision rule. The image shown on the right below is an SVM with a radial kernel applied to the same data.\n",
    "\n",
    "<img src=\"../images/non_linear_svm.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets get our hands dirty with some data analysis using Support vector machines. \n",
    "\n",
    "The **svm()** function can be used to fit a support vector classifier when the argument kernel=\"linear\" is used.  A `cost` argument allows us to specify the cost of a violation to the margin. When the cost argument is small, then the margins\n",
    "will be wide and many support vectors will be on the margin or will violate the margin. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin. We begin by generating the observations, which belong to two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the seed to reproduce the experiment results\n",
    "set.seed(1)\n",
    "\n",
    "# Generate a normal distribution of 40 values and form a matrix 'x' out of those values.\n",
    "x=matrix(rnorm(20*2), ncol=2)\n",
    "\n",
    "# Create a vector such that first 10 cells have value -1 and rest 10 cells have value 1.\n",
    "y=c(rep(-1,10), rep(1,10))\n",
    "\n",
    "# Increase the value of x1(first column in x) by 1. y==1 returns a vector of TRUE or FALSE based on condition. So `x[y==1,]` \n",
    "# implies it chooses last 10 rows of the matrix. Here we are increasing the values of x of last 10 rows by 1 so that they \n",
    "# are linearly separable. \n",
    "x[y==1,] = x[y==1,] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the classes are linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot x values. col=(3-y) will result in the vector of values {4,4...4,2,2,...} where first 10 are 4 and rest 10 are 2. This \n",
    "# will help in plotting the first values in blue and last 10 values in red color. \n",
    "plot(x, col =(3-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are not linearly separable. A red observation is grouped with blue observations. Fit a support vector classifier. In order for the svm() function to perform classification instead of SVM-based regression, encode the response as a factor variable. Create a data frame with the response coded as a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat=data.frame(x=x, y=as.factor(y))\n",
    "library(e1071)\n",
    "svmfit=svm(y ~., data=dat, kernel =\"linear\", cost =10,scale=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument scale=FALSE tells the svm() function not to scale each feature to have mean zero or standard deviation one; depending on the application, one might prefer to use scale=TRUE. Plot the support vector classifier obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Supply two arguements to plot() function, output of the call to svm() as well as the data used in the call to svm().\n",
    "plot(svmfit, dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region of feature space that is assigned to the −1 class is shown in light blue, and the region that is assigned to the +1 class is shown in purple. The decision boundary between the two classes is linear because of the argument kernel=\"linear\". Note that the second feature(X2) is plotted on the x-axis and the first feature(X1) is plotted on the y-axis, in contrast to the behavior of the usual plot() function in R. Here only one observation is misclassified. The support vectors are plotted as crosses and the remaining observations are plotted as circles. Also there are seven support vectors. We can determine their identities as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmfit$index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run summary() command to obtain some basic information about the support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(svmfit )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary() tells us that a linear kernel was used with cost=10, and there were seven support vectors. Four support vectors are in one class and three in the other. Fit the model with smaller cost value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmfit=svm(y~., data=dat, kernel=\"linear\", cost=0.1, scale=FALSE)\n",
    "plot(svmfit, dat)\n",
    "svmfit$index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a smaller value of the cost parameter we got a larger number of support vectors, because the margin is now wider. The svm() function does not explicitly output the coefficients of the linear decision boundary obtained when the support vector classifier is fit, nor does it output the width of the margin.\n",
    "\n",
    "\n",
    "The e1071 library includes a built-in function, `tune()`, to perform cross validation. By default, tune() performs ten-fold cross-validation on a set of models of interest. In order to use this function, pass in relevant information about the set of models that are under consideration. The following command compares SVMs with a linear kernel, using a range of values of the cost parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed (1)\n",
    "tune.out=tune(svm,y~.,data=dat, kernel =\"linear\", ranges =list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run summary() on tune.out to access the cross-validation errors for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(tune.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost=0.1 results in the lowest cross-validation error rate. The best model obtained can be accessed as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestmod = tune.out$best.model\n",
    "summary(bestmod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict() function can be used to predict the class label on a set of test observations, at any given value of the cost parameter. Generate a test data set as train data is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtest=matrix(rnorm(20*2), ncol =2)\n",
    "ytest=sample(c(-1,1), 20, rep=TRUE)\n",
    "xtest[ytest==1,] = xtest[ytest==1,] + 1\n",
    "testdat =data.frame(x=xtest, y=as.factor(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class labels of above test observations. The best model with cost=0.1 is used here in order to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypred=predict(bestmod, testdat)\n",
    "table(predict = ypred, truth=testdat$y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19 of the test observations are correctly classified with cost=0.1. Fit the support vector classifier and plot the resulting hyperplane, using a very large value of cost so that no observations are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat=data.frame(x=x,y=as.factor(y))\n",
    "svmfit =svm(y~.,data=dat ,kernel =\"linear\",cost =1e5)\n",
    "summary (svmfit )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No training errors were made and only three support vectors were used. However, we can see from the figure that the margin is very narrow (because the observations that are not support vectors, indicated as circles, are very close to the decision boundary). It seems likely that this model will perform poorly on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
