{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Discriminant Analysis(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Linear Discriminant Analysis or LDA is a technique to classify an object into one of two or more groups depending on a set of features that describes that object. It does so by assigning the object to the group with the highest conditional probability. Remember Bayes Rule? This is exactly that.\n",
    "\n",
    "LDA is an extension to logistic regression. Here the distribution of the predictors X is modeled separately in each of the\n",
    "response classes (i.e. given Y), and then Bayes’ theorem is used to flip these observations around into estimates for $Pr(Y = k\\ |\\ X = x)$.\n",
    "\n",
    "\n",
    "**Using Bayes’ Theorem for Classification**\n",
    "\n",
    "Suppose there are k(k>=2) classesk>=2 such that we want an observation to be classified into. The response variable Y can take on K possible distinct and unordered values. Let $π_k$ represent the prior probability that a randomly chosen observation comes from the kth class. This is the probability that a given observation is associated with the $k_{th}$ category of the response variable Y . Let $f_k(X) ≡ Pr(X = x|Y = k)$ denote the density function of $X$ for an observation that comes from the $k_{th}$ class. In other words, $f_k(x)$ is relatively large if there is a high probability that an observation in the $k_{th}$ class has X ≈ x, and fk(x) is small if it is very unlikely that an observation in the kth class has X ≈ x. Then Bayes’ theorem states that\n",
    "\n",
    "$$ Pr(Y = k\\ |\\ X = x) = \\frac{π_k f_k(x)}{\\sum_{l=1}^{K} π_l f_l(x)}$$\n",
    "\n",
    "If the above equation is represented using the notation used for logistic regression we will get something like $p_k(X) = Pr(Y = k\\ |\\ X)$. This means instead of directly computing $p_k(X)$, we can simply plug in estimates of $π_k$ and $f_k(X)$ into above complicated looking equation. Estimating $π_k$ is easy if we have a random sample of Ys from the population. We compute the fraction of the training observations that belong to the kth class. \n",
    "\n",
    "Estimating $f_k(X)$ is a bit more challenging, $p_k(x)$ is the posterior probability that an observation $X\\ =\\ x$ belongs to the $k_{th}$ class. That is, it is the probability that the observation belongs to the $k_{th}$ class, given the predictor value for that observation. Bayes classifier, classifies an observation to the class for which $p_k(X)$ is largest, has the lowest possible error rate out of all classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "We will evaluate the performance of LDA over logistic regression. Use Smarket dataset again to predict Direction. Fit a LDA model using the lda() function, which is part of the MASS library. Notice that the lda() syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. \n",
    "\n",
    "We are going to train the model on those observations that occurred before the year 2005, and predict for those observations during 2005.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "attach(Smarket)\n",
    "\n",
    "# The object train is a vector of 1,250 elements, corresponding to the observations in our data set. The elements of the \n",
    "# vector that correspond to observations that occurred before 2005 are set to TRUE as they satisfy the condition. \n",
    "# whereas those that correspond to observations in 2005 are set to FALSE. \n",
    "train = Year<2005\n",
    "\n",
    "# train is a Boolean vector, since its elements are TRUE and FALSE. So the TRUE and FALSE values corresponding to each row\n",
    "# will let you subset rows or columns of a matrix. For instance, the command Smarket[!train,] would pick out a submatrix of the\n",
    "# stock market dataset, corresponding to dates in 2005, since those are the ones for which the elements of train are FALSE and \n",
    "# `!` operator will reverse the elements of train vector.\n",
    "Smarket.2005= Smarket[!train,]\n",
    "\n",
    "# Check the dimensions of Smarket.2005\n",
    "dim(Smarket.2005)\n",
    "\n",
    "# Save the Direction values corresponding to 2005 dates.\n",
    "Direction.2005 = Direction[!train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "lda.fit=lda(Direction∼Lag1+Lag2 ,data=Smarket ,subset=train)\n",
    "lda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model output indicates that $\\hat{π}_1$ = 0.492 and $\\hat{π}_2$ = 0.508 i'e 49.2% of the observations correspond to days during which the market went down. These are the average probabilities of each predictor within each class, and are used by LDA as estimates of μk. \n",
    "\n",
    "The probabilities suggest there is a tendency for the last 2 days’ returns to be negative on days when the market increases and a tendency for the last 2 days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output are used to form the LDA decision rule. If −0.642×Lag1−0.514×Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline. The plot() function produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lda.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class. Finally, x contains the linear discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.pred=predict(lda.fit, Smarket.2005)\n",
    "names(lda.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.class =lda.pred$class\n",
    "table(lda.class ,Direction.2005)\n",
    "\n",
    "mean(lda.class == Direction.2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a 50% threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(lda.pred$posterior[ ,1]>=0.5)\n",
    "\n",
    "sum(lda.pred$posterior[,1]<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.pred$posterior[1:20 ,1]\n",
    "lda.class[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(lda.pred$posterior[,1]>.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
