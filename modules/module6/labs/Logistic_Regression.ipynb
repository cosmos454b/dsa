{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model discussed in module 5 assumes that the response variable $Y$ is quantitative (continous numerical value). However, what happens if the response variable is qualitative (categorical). For example, eye color is qualitative, taking values blue, brown, or green. The process of predicting qualitative responses is known as **classification**.\n",
    "\n",
    "Often the methods used for classification first predict the probability of each of the categories of a qualitative variable, as the basis for making the classification. In this sense, they also behave like regression methods.\n",
    "\n",
    "An example classification problem could be, an online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history etc. In this notebook we will continue our discussion on Logistic Regression using the `Default` data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is stated Linear Regression is not an ideal choice predicting a categorical variable. Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. There are three possible diagnoses: stroke, drug overdose, and epileptic seizure. We could consider encoding these values as a quantitative response variable, $Y$ , as follows:\n",
    "\n",
    "$$Y=\\begin{gather*}\n",
    "\\begin{cases}\n",
    "1,\\ if\\ Stroke;\\\\\n",
    "2,\\ if\\ Drug\\ Overdose;\\\\\n",
    "3,\\ if\\ Epileptic\\ Seizure.\\\\\n",
    "\\end{cases}\n",
    "\\end{gather*}$$\n",
    "\n",
    "Using this coding, least squares could be used to fit a linear regression model to predict Y on the basis of a set of predictors $X_1, . . .,X_p$. Unfortunately, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure, and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure. In practice there is no particular reason that this needs to be the case. For instance, one could choose an equally reasonable coding,\n",
    "\n",
    "$$Y=\\begin{gather*}\n",
    "\\begin{cases}\n",
    "1,\\ if\\ Epileptic\\ Seizure.\\\\\n",
    "2,\\ if\\ Stroke;\\\\\n",
    "3,\\ if\\ Drug\\ Overdose;\\\\\n",
    "\\end{cases}\n",
    "\\end{gather*}$$\n",
    "\n",
    "which would imply a totally different relationship among the three conditions. Each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations. In general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.\n",
    "\n",
    "If there are only two possibilities for the patient’s medical condition: stroke and drug overdose. We could then potentially use the dummy variable approach to code the response as follows:\n",
    "\n",
    "$$Y=\\begin{gather*}\n",
    "\\begin{cases}\n",
    "1,\\ if\\ Stroke;\\\\\n",
    "2,\\ if\\ Drug\\ Overdose;\\\\\n",
    "\\end{cases}\n",
    "\\end{gather*}$$\n",
    "\n",
    "We could then fit a linear regression to this binary response, and predict drug overdose if $\\hat{Y}$ >0.5 and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions. For a binary response with a 0/1 coding as above, regression by least squares does make sense.\n",
    "\n",
    "#### Below is a pictorial illustration of why linear regression is a bad choice for categorical variables. \n",
    "\n",
    "<img src=\"../images/linear_vs_logistic_regression.JPG\">\n",
    "\n",
    "Looking at the picture, left picture is the estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default(No or Yes). Right picture is the predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1.\n",
    "\n",
    "Considering the Default data set, the response default will be either Yes or No. Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category. For example, the probability of default given balance can be written as\n",
    "\n",
    "$$Pr(default = Yes\\ |\\ balance).$$\n",
    "\n",
    "The values of Pr(default = Yes | balance), which we abbreviate P(balance), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship we are interested in here is between $p(X) = Pr(Y = 1\\ |\\ X)$ and X?(predict Y=1 for a given value of X). The problem in using linear regression equation $p(X) = β_0 + β_1 X$, approach is for balances close to zero we predict a negative probability of default. If we were to predict for very large balances, we would get values bigger than 1. The predictions are not sensible, since true probability must fall between 0 and 1.\n",
    "\n",
    "To avoid this problem, we must model p(X) using a function that gives outputs between 0 and 1 for all values of X. Maximum likelihood is used in fitting a logistic regression model. The logistic function,\n",
    "\n",
    "$$p(X) = \\frac{e^{β_0+β_1X}}{1 + e^{β_0+β_1X}}$$\n",
    "\n",
    "Doing some mathematical derivations we arrive at below logistic function, for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. Above equation can also be written as \n",
    "\n",
    "$$\\frac{p(X)}{1 − p(X)} = e^{β_0+β_1X}$$\n",
    "\n",
    "The quantity $\\frac{p(X)}{[1−p(X)]}$is called the odds, and can take on any value between 0 and ∞. Values of the odds close to 0 and ∞ indicate very low and very high probabilities of default, respectively. By taking the logarithm of both sides of above equation, we arrive at \n",
    "\n",
    "$$log(\\frac{p(X)}{1 − p(X)}) = β_0 + β_1X$$\n",
    "\n",
    "The left-hand side is called the log-odds or logit. In a linear regression model, β1 gives the average change in Y associated with a one-unit increase in X. In contrast, in a logistic regression model, increasing X by one unit changes the log odds\n",
    "by $β_1$, or equivalently it multiplies the odds by ${e^{β_1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood is used to estimate coefficients. We seek estimates for β0 and β1 such that the predicted probability $p(x_i)$ of default for each individual, corresponds as closely as possible to the individual’s observed default status. In other words, we try to find $β_0$ and $β_1$ such that plugging these estimates into the model for p(X), i'e \n",
    "\n",
    "$$p(X) = \\frac{e^{β_0+β_1X}}{1 + e^{β_0+β_1X}}$$\n",
    "\n",
    "yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not.  \n",
    "\n",
    "Below table shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default=Yes using balance.\n",
    "\n",
    "|          |  Coefficient Std.| error   |Z-statistic |P-value  |\n",
    "|----------|------------------|---------|------------|---------|          \n",
    "|Intercept |−10.6513          |  0.3612 |−29.5       |  <0.0001|\n",
    "|balance   |0.0055            |  0.0002 | 24.9       |  <0.0001|\n",
    "\n",
    "We see that $\\hat{β_1} = 0.0055$ this indicates that an increase in balance is associated with an increase in the probability of default. To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0055\n",
    "units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Once the coefficients have been estimated, you compute the probability of default for any given credit card balance. For example, using the coefficient estimates given in above table, we predict that the default probability for an individual with a balance of $1, 000 is \n",
    "\n",
    "$$p(X) = \\frac{e^{β_0+β_1X}}{1 + e^{β_0+β_1X}} = \\frac{e^{-10.6513+0.0055*1000}}{1 + e^{-10.6513+0.0055*1000}} = 0.00576$$\n",
    "\n",
    "which is below $1%$. In contrast, the predicted probability of default for an individual with a balance of $2, 000 is much higher, and equals 0.586 or 58.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in Chapter 3, we can generalize (4.4) as follows:\n",
    "\n",
    "$$log(\\frac{p(X)}{1 − p(X)}) = β_0 + β_1X_1 + · · · + β_pX_p$$, where X = (X1, . . .,Xp) are p predictors\n",
    "\n",
    "Above equation can be written as \n",
    "\n",
    "$$p(X) = \\frac{e^{β_0+β_1X_1+···+β_pX_p}}{1 + e^{β_0+β_1X_1+···+β_pX_p}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for >2 Response Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the response variable has multiple categories like the medical condition example with levels stroke, drug overdose, epileptic seizure we would like to model both Pr(Y = stroke|X) and Pr(Y = drug overdose|X), with the remaining Pr(Y = epileptic seizure|X) = 1 − Pr(Y = stroke|X) − Pr(Y = drug overdose|X). The logistic regression model discussed before have multiple-class extensions, but in practice they are not used often for multiple class situation. The reason being there are other popular approaches for multiple-class classification like Linear Discriminnet Analysis(LDA). We will look more into LDA soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression model on Stock Market Data in ISLR package. This data set consists of percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, the percentage returns for each of the five previous trading days, Lag1 through Lag5 are recorded. The Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this date) are also recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"ISLR\", repo=\"https://cran.fhcrc.org/\")\n",
    "library(ISLR)\n",
    "\n",
    "# Check the names of columns present in the dataset.\n",
    "names(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the dimensions of the data.\n",
    "dim(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Direction variable is qualitative. cor() accepts numeric values only. So exclude Direction from the input to cor function\n",
    "cor(Smarket[,-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little\n",
    "correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume. By plotting the data we see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Telling R that we are going to use Smarket dataset in this data. You can refer to columns in Smarket without referring to the\n",
    "# Smarket everytime.\n",
    "attach(Smarket)\n",
    "plot(Smarket$Volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smarket has data spanning across 2001 to 2005. Subset the data into training and testing sets. Create a vector corresponding to the observations from 2001 through 2004. Use this vector to create two datasets of observations one with data from 2001 to 2004 and the other one containing 2005 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The object train is a vector of 1,250 elements, corresponding to the observations in our data set. The elements of the \n",
    "# vector that correspond to observations that occurred before 2005 are set to TRUE as they satisfy the condition. \n",
    "# whereas those that correspond to observations in 2005 are set to FALSE. \n",
    "train = Year<2005\n",
    "\n",
    "# train is a Boolean vector, since its elements are TRUE and FALSE. So the TRUE and FALSE values corresponding to each row\n",
    "# will let you subset rows or columns of a matrix. For instance, the command Smarket[!train,] would pick out a submatrix of the\n",
    "# stock market dataset, corresponding to dates in 2005, since those are the ones for which the elements of train are FALSE and \n",
    "# `!` operator will reverse the elements of train vector.\n",
    "Smarket.2005= Smarket[!train,]\n",
    "\n",
    "# Check the dimensions of Smarket.2005\n",
    "dim(Smarket.2005)\n",
    "\n",
    "# Save the Direction values corresponding to 2005 dates.\n",
    "Direction.2005 = Direction[!train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset argument below is providing the condition for what data to be selected from Smarket. If you are not sure whats \n",
    "# happening run below table command. \n",
    "glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial, subset=train)\n",
    "\n",
    "# Predicted probabilities of the stock market for each of the days in the test set that is, for the days in 2005\n",
    "glm.probs = predict(glm.fit, Smarket.2005, type=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the data for years from 2001 through 2004 using train vector. Use table function to see distribution of Year values\n",
    "# in the subset.\n",
    "table(subset(Smarket,train)$Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the model using only the dates before 2005, and fitted model is test on data with dates in 2005. The predictions for 2005 are in glm.probs. Compare them to the actual movements of the market over that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a vector glm.pred of size 252 to store the predictions. Assign the value `down` initially for entire vector \n",
    "glm.pred=rep(\"Down\" ,252)\n",
    "\n",
    "# Update the predictions in glm.pred to 'up' if predicted probability is greater than 0.5\n",
    "glm.pred[glm.probs >0.5]=\" Up\"\n",
    "\n",
    "# table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or \n",
    "# incorrectly classified.\n",
    "table(glm.pred, Direction.2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the names() function in order to find out what other pieces of information are stored in glm.fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the coef() function in order to access just the coefficients for this fitted model\n",
    "coef(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confint(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(glm.pred==Direction.2005)\n",
    "mean(glm.pred!= Direction.2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The != notation means not equal to and so the last command computes the test set error rate. The test error rate is 69%, which is worse than random guessing. Remove the variables that appear not to be helpful in predicting Direction. Using predictors that have no relationship with the response tends to cause a deterioration in the test error rate, since such predictors cause an increase in variance without a corresponding decrease in bias.\n",
    "\n",
    "Refit the logistic regression using just Lag1 and Lag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glm.fit=glm(Direction~Lag1+Lag2, data=Smarket, family=binomial, subset =train)\n",
    "glm.probs = predict(glm.fit, Smarket.2005, type=\"response\")\n",
    "\n",
    "glm.pred=rep(\"Down\", 252)\n",
    "glm.pred[glm.probs >0.5]=\"Up\"\n",
    "\n",
    "table(glm.pred, Direction.2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(glm.pred == Direction.2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "106/(106+76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good compared to previous model. Now 56% of the daily movements have been correctly predicted. The confusion matrix suggests that on days when logistic regression predicts that the market will decline, it is only correct 50% of the time. However, on days when it predicts an increase in the market, it has a 58% accuracy rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
