{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "In this practice, we will create some toy data sets to demonstrate the idea of linear separability and apply LDA to them to see how it performs on data sets with different characteristics.\n",
    "\n",
    "Let's create a data set with linearly separable classes. We'll have only two dimensions to make it easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember; a different data set will be created randomly every time you run this code.\n",
    "# A data set with 150 samples where class means are 3,3 and 8,8 with a sigma of 1. \n",
    "\n",
    "# 75 (x,y) pairs with (3,3) mean\n",
    "p1<-cbind(rnorm(n=75,mean=3,sd=1),rnorm(n=75,mean=3,sd=1))\n",
    " # 75 (x,y) pairs with (8,8) mean\n",
    "p2<-cbind(rnorm(n=75,mean=8,sd=1),rnorm(n=75,mean=8,sd=1))\n",
    "# first 75 will be class 1, last 75 will be class -1\n",
    "t<-c(rep(1,75),rep(-1,75)) \n",
    "\n",
    "# Now create the data frame\n",
    "data1<-as.data.frame(cbind(rbind(p1,p2),t))\n",
    "names(data1)<-c(\"X\",\"Y\",\"class\")\n",
    "\n",
    "# you can also read from the file instead. \n",
    "# data1 <- read.csv(\"../../../datasets/toydata/data1.csv\",header=TRUE)\n",
    "\n",
    "str(data1)\n",
    "# Visualize the data\n",
    "library(ggplot2)\n",
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) #+ theme(legend.position=\"none\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two linearly separable classes in the data set that are labeled as \"+1\" and \"-1\". Here are two random decision boundaries that do pretty good job in separating the classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl1 + geom_abline(intercept=9.5, slope=-1)+geom_abline(intercept=11, slope=-1.1) + theme(legend.position=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's find the decision boundary of LDA for this data set\n",
    "\n",
    "library(MASS)\n",
    "ldafit1 <- lda(class ~ X + Y, data=data1)\n",
    "ldafit1\n",
    "plot(ldafit1)\n",
    "# Run the model on the same data that it was trained with. \n",
    "pred <- predict(ldafit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the values the linear discriminant LD1 takes for the members of each class (class \"-1\" and class \"1\"). The LD1 is a *linear* combination of X and Y: \n",
    "\n",
    "$LD1 = -0.6458231X - 0.7221019Y $\n",
    "\n",
    "It can separate two classes; they are *linearly separable*. Let's look at the confusion table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a confusion table.\n",
    "conftable1 <- table(pred$class, data1$class)\n",
    "conftable1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no confusion between classes; accuracy is 100% (occasionally, you can have a few misclassified points; that is because a random new data set will be created every time you run the code, and a few points may end up too close to the other class).\n",
    "\n",
    "We also want to see the actual decision boundary of the model. To plot the LDA decision boundary, \n",
    "we'll have to do a bit of extra work, don't worry if you don't fully understand\n",
    "what's going on in the next few lines. We basically create a grid and make a prediction for each point of the grid \n",
    "and then draw a contour plot of the grid on top of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# next three lines are for plotting boundary.\n",
    "contour_data <- data.frame(expand.grid(X=seq(0,15,length=300),Y=seq(0,15,length=300)))\n",
    "lda_predict1 <- data.frame(contour_data, as.numeric(predict(ldafit1,contour_data)$class))\n",
    "names(lda_predict1)[3] <- \"pred\"\n",
    "pl1 + stat_contour(aes(x=X, y=Y, z=pred),data=lda_predict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, LDA does a good job if the classes are linearly seperable. Let's create another data set \n",
    "where they can't be separated without making some errors. Here, the samples of different classes will be \n",
    "very closely located so that you can't find a linear separation without misclassifying some of them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is how to create the data; but let's read it from the file instead.\n",
    "#p1<-cbind(rnorm(n=75,mean=4,sd=1),rnorm(n=75,mean=4,sd=1))\n",
    "#p2<-cbind(rnorm(n=75,mean=6,sd=1),rnorm(n=75,mean=6,sd=1))\n",
    "#t<-c(rep(1,75),rep(-1,75))\n",
    "#data2<-as.data.frame(cbind(rbind(p1,p2),t))\n",
    "#names(data2)<-c(\"X\",\"Y\",\"class\")\n",
    "##\n",
    "\n",
    "# read from the file \n",
    "data2 <- read.csv(\"../../../datasets/toydata/data2.csv\",header=TRUE)\n",
    "\n",
    "# Visualize the data\n",
    "pl2 <- ggplot(data2, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again, run the LDA on this data set\n",
    "ldafit2 <- lda(<what goes in here>)\n",
    "plot(ldafit2)\n",
    "ldafit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, you can see that LD1 does not produce a 100% accurate classification; there is an overlap between classes. This means that some of the samples of a class will be misclassified as the other class; these samples will be on the wrong side of the decision boundary. Let's see that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with. \n",
    "pred <- predict(ldafit2)\n",
    "conftable2 <- table(<what goes in here>)\n",
    "conftable2\n",
    "\n",
    "contour_data <- data.frame(expand.grid(X=seq(2,8,length=300),Y=seq(2,8,length=300)))\n",
    "lda_predict2 <- data.frame(contour_data, as.numeric(predict(ldafit2,contour_data)$class))\n",
    "names(lda_predict2)[3] <- \"pred\"\n",
    "pl2 + stat_contour(aes(x=X, y=Y, z=pred),data=lda_predict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier does a good job, but not without mistakes. Let's compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (paste(\"accuracy = \",sum(diag(conftable2))/length(pred$class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create another data set where classes do not have linear separation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This data set can't be separated by a linear decision boundary, \n",
    "# but we can find a nonlinear decision boundary that can do a pretty \n",
    "# good job. \n",
    "\n",
    "#p1<-cbind(rnorm(n=25,mean=1,sd=0.3),rnorm(n=25,mean=1,sd=0.3))\n",
    "#p2<-cbind(rnorm(n=25,mean=2,sd=0.3),rnorm(n=25,mean=2,sd=0.3))\n",
    "#p3<-cbind(rnorm(n=25,mean=3,sd=0.3),rnorm(n=25,mean=3,sd=0.3))\n",
    "#p4<-cbind(rnorm(n=25,mean=4,sd=0.3),rnorm(n=25,mean=2,sd=0.3))\n",
    "#p5<-cbind(rnorm(n=25,mean=5,sd=0.3),rnorm(n=25,mean=1,sd=0.3))\n",
    "#p6<-cbind(rnorm(n=125,mean=3,sd=0.5),rnorm(n=125,mean=1.5,sd=0.2))\n",
    "#t<-c(rep(1,125),rep(-1,125))\n",
    "#data3<-as.data.frame(cbind(rbind(p1,p2,p3,p4,p5,p6),t))\n",
    "#names(data3)<-c(\"X\",\"Y\",\"class\")\n",
    "\n",
    "# read from file instead\n",
    "data3 <- read.csv(\"../../../datasets/toydata/data3.csv\",header=TRUE)\n",
    "\n",
    "pl3 <- ggplot(data3, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again, run the LDA on this data set\n",
    "ldafit3 <- <what goes in here>\n",
    "plot(ldafit3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that group (class) \"1\" response of the LD1 is all over the place; this means class \"1\" samples are on both sides of the decision boundary. Class \"-1\" seems to be mostly on one side of the decision boundary but still has many misclassifications. There is no good linear separation; these classes are *not linearly separable*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with. \n",
    "pred <- predict(ldafit3)\n",
    "conftable3 <- <what goes in here>\n",
    "conftable3\n",
    "\n",
    "contour_data <- data.frame(expand.grid(X=seq(0,6,length=300),Y=seq(0,3,length=300)))\n",
    "lda_predict3 <- data.frame(contour_data, as.numeric(predict(ldafit3,contour_data)$class))\n",
    "names(lda_predict3)[3] <- \"pred\"\n",
    "\n",
    "pl3 + stat_contour(aes(x=X, y=Y, z=pred),data=lda_predict3) + theme(legend.position=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is much lower; there is a lot of \"confusion\" between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (paste(\"accuracy = \",<what goes in here>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create yet another data set; this one is known as the \"XOR pattern\". We will have two classes that are linearly nonseparable even though their samples seem to be nicely separated in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And this is yet another toy data set where we can't find any linear separation without \n",
    "# making gross misclassification. This is the so called \"XOR pattern\". \n",
    "#p11<-cbind(rnorm(n=25,mean=1,sd=0.3),rnorm(n=25,mean=1,sd=0.3))\n",
    "#p12<-cbind(rnorm(n=25,mean=-1,sd=0.3),rnorm(n=25,mean=1,sd=0.3))\n",
    "#p13<-cbind(rnorm(n=25,mean=-1,sd=0.3),rnorm(n=25,mean=-1,sd=0.3))\n",
    "#p14<-cbind(rnorm(n=25,mean=1,sd=0.3),rnorm(n=25,mean=-1,sd=0.3))\n",
    "#t<-c(rep(1,50),rep(-1,50))\n",
    "#data4<-as.data.frame(cbind(rbind(p11,p13,p12,p14),t))\n",
    "#names(data4)<-c(\"X\",\"Y\",\"class\")\n",
    "\n",
    "# read from file instead\n",
    "data4 <- read.csv(\"../../../datasets/toydata/data4.csv\",header=TRUE)\n",
    "\n",
    "pl4 <- ggplot(data4, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOW YOUR TURN: find LDA model and compute confusion table and accuracy\n",
    "<what goes in here>\n",
    "\n",
    "# display the decision boundary\n",
    "contour_data <- data.frame(expand.grid(X=seq(-2,2,length=300),Y=seq(-2,2,length=300)))\n",
    "lda_predict4 <- data.frame(contour_data, as.numeric(predict(ldafit4,contour_data)$class))\n",
    "names(lda_predict4)[3] <- \"pred\"\n",
    "\n",
    "pl4 + stat_contour(aes(x=X, y=Y, z=pred),data=lda_predict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (paste(\"accuracy = \",<what goes in here>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much the worst case scenario; the classifier does not do any better than a \"coin toss\" (50% accuracy). The LD1 response distribution for both classes show a \"bimodal\" distribution (two peaks) on either side of the decision boundary. Linear models can not deal with this data set. We'll need *nonlinear* models to classify this data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
