{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "In this practice, we will use the same data sets we have used in [Linear Discriminant Analysis practice notebook](Linear_Discriminant_Analysis.ipynb) with the support vector machine classifiers. Take a look at that practice first if you haven't done so yet. \n",
    "\n",
    "We will start with the first data set that has two linearly separable classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 <- read.csv(\"../../../datasets/toydata/data1.csv\",header=TRUE)\n",
    "\n",
    "# For SVM, we need to make sure class is a factor.\n",
    "data1$class <- factor(data1$class)\n",
    "str(data1)\n",
    "# Visualize the data\n",
    "library(ggplot2)\n",
    "pl1 <- ggplot(data1, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) #+ theme(legend.position=\"none\")\n",
    "pl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes labeled as \"0\" and \"1\" are *linearly separable*; we can draw a linear decision boundary to separate them. Let's support vector machine (SVM) to this data set. We will use the library \"e1071\" for SVMs, and \"caret\" library (classification and regression training) that has nice functions to deal with several aspects of classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(e1071)\n",
    "library(caret)\n",
    "svm_model1 = svm(class ~ ., data=data1, kernel=\"linear\", cost=10, scale=FALSE)\n",
    "summary(svm_model1)\n",
    "plot(svm_model1,data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a SVM with a *linear* kernel; and it learned a linear decision boundary. \n",
    "What you see as marked \"X\" in the above plot are the data points that serve as support vectors; \n",
    "there are three of them. Let's compute the confusion matrix, and as expected, we'll get perfect accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=predict(svm_model1, data1[,-3])\n",
    "conftable1=table(predict=pred, class=data1$class)\n",
    "conftable1\n",
    "\n",
    "# caret has a function to compute confusion matrix and \n",
    "# other things such as accuracy, sensitivity, specificity, etc.\n",
    "confusionMatrix(data=pred, data1$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the same to the second data set; **it's your turn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 <- read.csv(\"../../../datasets/toydata/data2.csv\",header=TRUE)\n",
    "\n",
    "data2$class <- factor(data2$class)\n",
    "# Visualize the data\n",
    "pl2 <- ggplot(data2, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, you can see that there is an overlap between classes. This means that some of the samples of a class will be misclassified as the other class; these samples will be on the wrong side of the decision boundary. Let's see that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model2 = svm(<what goes in here>)\n",
    "summary(svm_model2)\n",
    "plot(<what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the confusion table. **Again, it's your turn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=predict(svm_model2, data2[,-3])\n",
    "conftable2=table(<what goes in here>)\n",
    "conftable2\n",
    "# or do this\n",
    "confusionMatrix(<what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the same to the third data set where classes are not linearly separable. \n",
    "**It's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3 <- read.csv(\"../../../datasets/toydata/data3.csv\",header=TRUE)\n",
    "\n",
    "data3$class <- <what goes in here>\n",
    "# Visualize the data\n",
    "pl3 <- ggplot(data3, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model3 = svm(<what goes in here>)\n",
    "summary(svm_model3)\n",
    "plot(<what goes in here>)\n",
    "pred=predict(<what goes in here>)\n",
    "conftable3=table(<what goes in here>)\n",
    "conftable3\n",
    "# or do this\n",
    "confusionMatrix(<what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty bad; SVM with a linear kernel can't classify this data set. Luckily there are nonlinear kernels that we can use with SVM. Let's try a **radial basis function (RBF)** kernel with SVM, it's one of the most used kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model_rbf = svm(class ~ ., data=data3, kernel=\"radial\", cost=10, scale=FALSE)\n",
    "summary(svm_model_rbf)\n",
    "plot(svm_model_rbf,data3)\n",
    "pred=predict(svm_model_rbf, data3[,-3])\n",
    "conftable_rbf=table(predict=pred, class=data3$class)\n",
    "conftable_rbf\n",
    "\n",
    "confusionMatrix(data=pred, data3$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it does a pretty good job in classifying data; the decision boundary does not have to be linear any more, so this SVM model learns a boundary from the data that can be represented by radial basis functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is a potential problem here. We trained and tested our model with the same exact data set. This can cause *memorization*; the model does not learn a decision boundary, it memorizes a boundary for this particular data set. We don't know how it will perform on the *new, unseen* observations. One of the most important aspects of learning algorithms is their ability to *generalize*; that is, to learn decision boundaries that are generalized enough to do well on unseen data. \n",
    "\n",
    "So, we need to separate our data set into a training subset and a testing subset; and train the model with the training set and test it (predict and compute the accuracy) with the testing set. There are different ways of it; we'll do two moethods: 1) split test, 2) cross-validation. \n",
    "\n",
    "### Split test \n",
    "Split test is simply splitting the data into training and testing sets; we can use 65% of the data for training, and the rest for testing (usually training set is larger than the testing set). \n",
    "\n",
    "Here's how we do it with caret library's functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an 65%/35% train/test split of the dataset\n",
    "split=0.65\n",
    "# create indices that belong to the training set\n",
    "trainIndex <- createDataPartition(data3$class, p=split, list=FALSE)\n",
    "# pick the samples with those indices, they will be training set\n",
    "train_set <- data3[trainIndex,]\n",
    "# pick the rest of the samples, they will be testing set\n",
    "test_set  <- data3[-trainIndex,]\n",
    "# train a svm model with training set only\n",
    "svm_model_rbf2 = svm(class ~ ., data=train_set, kernel=\"radial\", cost=10, scale=FALSE)\n",
    "summary(svm_model_rbf2)\n",
    "plot(svm_model_rbf2,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now predict both training set and testing set outcomes of the model and compare.\n",
    "predtr=predict(svm_model_rbf2, train_set[,-3])\n",
    "predts=predict(svm_model_rbf2, test_set[,-3])\n",
    "\n",
    "confusionMatrix(data=predtr, train_set$class)\n",
    "confusionMatrix(data=predts, test_set$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the model is expected to do a better job on predictions for the training set; because that's what it has learned. If it has generalized well enough, it should also produce good performance for the testing set. The problem still continues though; how do we know that this particular training set represents the class distribution well enough? We need to repeat this split test a number of times and compute the mean and standard deviation of the accuracy. We can do this with more random splittings, or we can use the cross-validation approach.\n",
    "\n",
    "### k-fold cross-validation\n",
    "k-fold cross-validation splits the data set into *k* subsets (folds) and then picks one subset for testing and trains the model with the remaining *k-1* subsets. It does this for each fold; so for k=10, it'll end up doing 10 training and testing sessions. \n",
    "\n",
    "We can use cross validation to **tune the parameters** of the SVM model to get a better accuracy without the danger of memorization. We will use svm tuning functions for that. Let's see how. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start the random number generator with some arbitrary seed\n",
    "set.seed(42)\n",
    "# Setup for cross validation:\n",
    "# sampling=\"cross\" for cross-validation, \n",
    "# cross=10 for 10-fold,\n",
    "# retain the best model and save the performance measures\n",
    "tctrl <- tune.control(sampling=\"cross\", cross=10, best.model=TRUE, performances=TRUE)                     \n",
    " \n",
    "# now run the tune function to tune the parameters of the model \n",
    "# tune function will try to find the best parameters (gamma and cost), that means the parameters with the smallest error.\n",
    "# it will try different gamma and cost values given as arguments (e.g. cost=1, cost=10, cost=100, etc.)\n",
    "tuned_params_cv <- tune(svm, class ~ ., data=data3, kernel=\"radial\", ranges=list(gamma=10^(-6:-1), cost=10^(0:2)), tunecontrol=tctrl)\n",
    "summary (tuned_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gamma=0.1 and cost=100 are the best parameters\n",
    "# now train a model with the tuned parameters.\n",
    "svm_model_rbf_cv = svm(class ~ ., data=data3, kernel=\"radial\", cost=100, gamma=0.1, scale=FALSE)\n",
    "summary(svm_model_rbf_cv)\n",
    "plot(svm_model_rbf_cv,data3)\n",
    "# find predictions \n",
    "pred=predict(svm_model_rbf_cv, data3[,-3])\n",
    "confusionMatrix(data=pred, data3$class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We should really use the training and testing sets here. Even though we did cross validation, it was for \n",
    "# parameter tuning, let's train a model with the training set. \n",
    "svm_model_rbf_cv2 = svm(class ~ ., data=train_set, kernel=\"radial\", cost=100, gamma=0.1, scale=FALSE)\n",
    "summary(svm_model_rbf_cv2)\n",
    "plot(svm_model_rbf_cv2,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compare training and testing set accuracies. \n",
    "predtr=predict(svm_model_rbf_cv2, train_set[,-3])\n",
    "predts=predict(svm_model_rbf_cv2, test_set[,-3])\n",
    "confusionMatrix(data=predtr, train_set$class)\n",
    "confusionMatrix(data=predts, test_set$class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We find the parameters with cross validation and use separate sets for training and testing, this way we have an accurate picture of classification performance of the model. \n",
    "\n",
    "Now, apply the same ideas to the \"XOR pattern\" data set where we have two classes that are linearly nonseparable even though their samples seem to be nicely separated in the plot. \n",
    "\n",
    "**Again, it's your turn.** First do parameter tuning with 10-fold cross validation and then train model and test it just like above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data4 <- read.csv(\"../../../datasets/toydata/data4.csv\",header=TRUE)\n",
    "\n",
    "data4$class <- <what goes in here>\n",
    "# Visualize the data\n",
    "pl4 <- ggplot(data4, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "pl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tctrl <- tune.control(<what goes in here>)                     \n",
    "tuned_params_cv <- tune(svm, class ~ ., data=data3, kernel=\"radial\", ranges=list(gamma=10^(-6:-1), cost=10^(0:2)), tunecontrol=tctrl)\n",
    "summary (tuned_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training and testing sets\n",
    "split=0.65\n",
    "trainIndex <- createDataPartition(<what goes in here>)\n",
    "train_set <- data4[<what goes in here>]\n",
    "test_set  <- <what goes in here>\n",
    "\n",
    "# train a svm model with training set only\n",
    "svm_model_4 = svm(<what goes in here>)\n",
    "summary(svm_model_4)\n",
    "plot(svm_model_4,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predtr=predict(<what goes in here>)\n",
    "predts=<what goes in here>\n",
    "confusionMatrix(<what goes in here>)\n",
    "confusionMatrix(data=predts, <what goes in here>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you should convert categorical variables to factors (just like we did in the above examples with the class variable) when using SVM. \n",
    "\n",
    "Here are some links to dig deeper: \n",
    "\n",
    "[A tour of machine learning algorithms](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)\n",
    "\n",
    "[Comparing machine learning classifiers](http://tjo-en.hatenablog.com/entry/2014/01/06/234155)\n",
    "\n",
    "[Training and testing concepts 1](http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/)\n",
    "\n",
    "[Training and testing concepts 2](http://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
